<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Noise Health</journal-id><journal-id journal-id-type="iso-abbrev">Noise Health</journal-id><journal-id journal-id-type="publisher-id">NH</journal-id><journal-title-group><journal-title>Noise &#x00026; Health</journal-title></journal-title-group><issn pub-type="ppub">1463-1741</issn><issn pub-type="epub">1998-4030</issn><publisher><publisher-name>Medknow Publications &#x00026; Media Pvt Ltd</publisher-name><publisher-loc>India</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">29319009</article-id><article-id pub-id-type="pmc">5771057</article-id><article-id pub-id-type="publisher-id">NH-19-254</article-id><article-id pub-id-type="doi">10.4103/nah.NAH_83_16</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>Cognitive Function Predicts Listening Effort Performance During Complex Tasks in Normally Aging Adults</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Harvey</surname><given-names>Jennine</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"/></contrib><contrib contrib-type="author"><name><surname>von Hapsburg</surname><given-names>Deborah</given-names></name><xref ref-type="aff" rid="aff2">1</xref></contrib><contrib contrib-type="author"><name><surname>Seeman</surname><given-names>Scott</given-names></name><xref ref-type="aff" rid="aff1"/></contrib></contrib-group><aff id="aff1">Communication Sciences and Disorders, Illinois State University, Normal, IL, USA</aff><aff id="aff2"><label>1</label>Department of Audiology &#x00026; Speech Pathology, University of Tennessee, Health Science Center, Knoxville, TN, USA</aff><author-notes><corresp id="cor1"><bold>Address for correspondence:</bold> Dr. Jennine Harvey, Communication Sciences and Disorders, Illinois State University, Normal, IL 61790, USA. E-mail: <email xlink:href="jmharv2@ilstu.edu">jmharv2@ilstu.edu</email></corresp></author-notes><pub-date pub-type="ppub"><season>Nov-Dec</season><year>2017</year></pub-date><volume>19</volume><issue>91</issue><fpage>254</fpage><lpage>262</lpage><permissions><copyright-statement>Copyright: &#x000a9; 2017 Noise &#x00026; Health</copyright-statement><copyright-year>2017</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by-nc-sa/3.0"><license-p>This is an open access article distributed under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License, which allows others to remix, tweak, and build upon the work non-commercially, as long as the author is credited and the new creations are licensed under the identical terms.</license-p></license></permissions><abstract><sec id="st1"><title>Purpose:</title><p>This study examines whether cognitive function, as measured by the subtests of the Woodcock&#x02013;Johnson III (WCJ-III) assessment, predicts listening-effort performance during dual tasks across the adults of varying ages.</p></sec><sec id="st2"><title>Materials and Methods:</title><p>Participants were divided into two groups. Group 1 consisted of 14 listeners (number of females&#x02009;=&#x02009;11) who were 41&#x02013;61 years old [mean&#x02009;=&#x02009;53.18; standard deviation (SD)&#x02009;=&#x02009;5.97]. Group 2 consisted of 15 listeners (number of females&#x02009;=&#x02009;9) who were 63&#x02013;81 years old (mean&#x02009;=&#x02009;72.07; SD&#x02009;=&#x02009;5.11). Participants were administered the WCJ-III Memory for Words, Auditory Working Memory, Visual Matching, and Decision Speed subtests. All participants were tested in each of the following three dual-task experimental conditions, which were varying in complexity: (1) auditory word recognition&#x02009;+&#x02009;visual processing, (2) auditory working memory (word)&#x02009;+&#x02009;visual processing, and (3) auditory working memory (sentence)&#x02009;+&#x02009;visual processing in noise.</p></sec><sec id="st3"><title>Results:</title><p>A repeated measures analysis of variance revealed that task complexity significantly affected the performance measures of auditory accuracy, visual accuracy, and processing speed. Linear regression revealed that the cognitive subtests of the WCJ-III test significantly predicted performance across dependent variable measures.</p></sec><sec id="st4"><title>Conclusion:</title><p>Listening effort is significantly affected by task complexity, regardless of age. Performance on the WCJ-III test may predict listening effort in adults and may assist speech-language pathologist (SLPs) to understand challenges faced by participants when subjected to noise.</p></sec></abstract><kwd-group><title>Keywords</title><kwd>Aging</kwd><kwd>cognitive function</kwd><kwd>dual-task</kwd><kwd>listening effort</kwd><kwd>task complexity</kwd><kwd>working memory</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-1"><title>I<sc>ntroduction</sc></title><p>Recent research investigating listening effort during the performance of dual tasks has established that cognition and hearing function are tightly linked.[<xref rid="ref1" ref-type="bibr">1</xref><xref rid="ref2" ref-type="bibr">2</xref><xref rid="ref3" ref-type="bibr">3</xref><xref rid="ref4" ref-type="bibr">4</xref><xref rid="ref5" ref-type="bibr">5</xref>] Specifically, working memory, attention, and processing speed have been identified as the dominant cognitive processes, or mental skills, which support auditory processing for speech perception.[<xref rid="ref6" ref-type="bibr">6</xref><xref rid="ref7" ref-type="bibr">7</xref><xref rid="ref8" ref-type="bibr">8</xref><xref rid="ref9" ref-type="bibr">9</xref>] Although cognitive and auditory resources contribute to the skills utilized to process auditory information, cognitive screens to determine function have not been routinely adopted in audiologic clinics, nor have they been consistently employed in research on cognition and listening effort. The purpose of this study was to explore whether cognitive screenings regularly performed by speech language pathologists could be used to predict listening effort in older listeners. Exploring the link between cognitive assessments (routinely used by speech-language pathologist, SLPs) and listening effort may contribute to our understanding of how noise and task difficulty during dual-task performance may influence older listeners.</p><p>Listening effort refers specifically to the mental skills, including attention, needed to understand speech.[<xref rid="ref1" ref-type="bibr">1</xref><xref rid="ref2" ref-type="bibr">2</xref><xref rid="ref4" ref-type="bibr">4</xref><xref rid="ref10" ref-type="bibr">10</xref><xref rid="ref11" ref-type="bibr">11</xref><xref rid="ref12" ref-type="bibr">12</xref>] Dual-task paradigms (DTPs) have been used successfully to study the complex and contradictory relationship between cognition and listening effort.[<xref rid="ref1" ref-type="bibr">1</xref><xref rid="ref2" ref-type="bibr">2</xref><xref rid="ref4" ref-type="bibr">4</xref><xref rid="ref11" ref-type="bibr">11</xref><xref rid="ref12" ref-type="bibr">12</xref><xref rid="ref13" ref-type="bibr">13</xref><xref rid="ref14" ref-type="bibr">14</xref><xref rid="ref15" ref-type="bibr">15</xref><xref rid="ref16" ref-type="bibr">16</xref><xref rid="ref17" ref-type="bibr">17</xref><xref rid="ref18" ref-type="bibr">18</xref>] Although the employment of dual-task methodology remains highly variable across fields and warrants further exploration for the most efficacious, hierarchical use, the purpose of the current study is to explore a possible predictive relationship between cognitive function and dual-task performance.</p><p>Gosselin and Gagn&#x000e9;[<xref rid="ref1" ref-type="bibr">1</xref>] used DTPs to study listening effort in noise across the young (18&#x02013;33 years old) and older (64&#x02013;76 years old) groups of listeners. All participants demonstrated hearing and cognitive functions within normal limits during screenings. Cognitive function was established using the Montreal Cognitive Assessment (MoCA). Participants were then administered the following three experimental conditions: (1) &#x0201c;closed-set sentence recognition in noise&#x0201d; (primary task), (2) &#x0201c;tactile pattern recognition in quiet&#x0201d; (secondary task), and (3) both the primary and secondary tasks completed simultaneously (dual task).<sup>[1 (p. 789)]</sup> During the practice session, noise levels were set for 80% performance accuracy. Results indicated that older adults exhibited increased listening effort when compared to younger adults using the dual-task methodology. Specifically, results revealed decreased performance for both accuracy and processing speed across task complexity in the older adult group. These findings suggest that older adults use more cognitive resources to identify speech, with decreased resources for additional tasks.[<xref rid="ref1" ref-type="bibr">1</xref>] While researchers did establish participant cognitive function via the MoCA, they did not utilize a screening tool designed to assess the specific cognitive skills activated during speech in noise tasks, nor did they explore whether the cognitive function predicted listening effort performance. Specifically, the MoCA is a screening tool for mild cognitive dysfunction.[<xref rid="ref19" ref-type="bibr">19</xref>] The MoCA is designed to screen general cognitive status, and, therefore, does not provide a rigorous assessment of the cognitive skills of working memory, sustained attention, and processing speed attention,[<xref rid="ref6" ref-type="bibr">6</xref><xref rid="ref8" ref-type="bibr">8</xref>] which are known to be activated during speech perception in noise (SPIN).</p><p>A more recent study by Degeest <italic>et al</italic>.[<xref rid="ref20" ref-type="bibr">20</xref>] investigated the effects of age on listening effort using dual-task methodologies in 60 normal-hearing listeners ranging in age from 20 to 77 years. As in the Gosselin and Gagn&#x000e9;[<xref rid="ref1" ref-type="bibr">1</xref>] study, participants were screened for normal cognitive function using the MoCA; however, only participants over the age of 60 years were subjected to this screening. Participants then completed the following three experimental conditions: (1) a primary speech-recognition task in noise, (2) a secondary visual memory task, and (3) the primary and secondary tasks concurrently. Results showed that not only does listening effort increase with age, but listening effort performance begins to show greater challenge starting in the fourth decade of life, even when hearing function is accounted for. Because Degeest <italic>et al</italic>.[<xref rid="ref20" ref-type="bibr">20</xref>] only screened cognition in participants over the age of 60 years, it is unclear whether changes in cognitive function may have been influencing the listeners below that age. Because normal cognitive decline with age can begin as early as 45 years,[<xref rid="ref21" ref-type="bibr">21</xref>] some participants may have demonstrated undocumented typical cognitive decline with age. Therefore, increases in listening effort in younger individuals may be due to undocumented cognitive decline. Further, the MoCA is a general cognitive screener and is not designed to assess cognitive skills contributing to listening effort.</p><p>The purpose of the current study is to examine the relationship between cognitive subtests and performance during dual tasks. Specifically, do the Woodcock&#x02013;Johnson III (WCJ-III) subtests of Memory for Words, Auditory Working Memory, Visual Matching, and Decision Speed predict auditory accuracy, visual accuracy, and visual reaction time during complex tasks? We hypothesized that cognitive function would predict performance across age groups. If cognitive function predicts performance during dual task, this may provide insight regarding the types of tasks that should be included in cognitive screeners for audiologic and speech pathology services.</p></sec><sec sec-type="materials|methods" id="sec1-2"><title>M<sc>aterials and</sc> M<sc>ethods</sc></title><sec id="sec2-1"><title>Participants</title><p>Twenty-nine native speakers of American English were divided into two groups based on their age. Group 1 included 14 participants (number of females&#x02009;=&#x02009;11; number of males&#x02009;=&#x02009;4), aged 41&#x02013;61 years old [mean&#x02009;=&#x02009;53.18; standard deviation (SD)&#x02009;=&#x02009;5.97]. Group 2 included 15 participants (number of females&#x02009;=&#x02009;9; number of males&#x02009;=&#x02009;6), aged 63&#x02013;81 years old (mean&#x02009;=&#x02009;72.07; SD&#x02009;=&#x02009;5.11). All participants demonstrated normal to near-normal hearing and normal middle ear function on audiometric assessment. Normal to near-normal hearing criterion included thresholds less than or equal to 35&#x02009;dB HL from 250 to 3000&#x02009;Hz.[<xref rid="ref8" ref-type="bibr">8</xref>] The mean four-frequency pure tone average (PTA) for the right and left ears for Group 1 were 17.18&#x02009;dB HL (SD&#x02009;=&#x02009;6.11), and 15.98&#x02009;dB HL (SD&#x02009;=&#x02009;6.60), respectively. The mean four-frequency PTA for the right and left ears for Group 2 were 22.67&#x02009;dB HL (SD&#x02009;=&#x02009;5.89), and 21.42&#x02009;dB HL (SD&#x02009;=&#x02009;7.62), respectively.</p><p>Cognitive function was examined utilizing a screening battery of standardized cognitive subtests compiled from the WCJ-III[<xref rid="ref22" ref-type="bibr">22</xref>] test of cognitive abilities. The four subtests (Memory for Words, Auditory Working Memory, Decision Speed, and Visual Matching) provided a baseline for the functions of processing speed and working memory across auditory and visual tasks. The Memory for Words subtest was utilized to examine participants&#x02019; short-term memory, specifically auditory memory span. For this subtest, participants repeat aurally presented sequences of up to seven unrelated words in the same order they were presented. The Auditory Working Memory subtest also examined short-term memory, specifically working memory and cognitive processes responsible for the recoding of acoustic stimuli.[<xref rid="ref23" ref-type="bibr">23</xref>] The task involves retaining two types of information (words and numbers) presented orally in a mixed order and then reordering that information and repeating first the words and then the numbers. The Visual Matching subtest is used to assess perceptual speed for visual stimuli. Perceptual speed involves making comparisons on the basis of rapid visual searches. In this subtest, participants are presented with 60 printed sets of six numbers and are directed to identify (and circle) two identical numbers within each set, within 3&#x02009;min. The six number sets range from single- to triple-digit numbers. Finally, the Decision Speed subtest is a timed task that tests processing speed, specifically semantic processing speed (symbolic and semantic comparisons).[<xref rid="ref23" ref-type="bibr">23</xref>] In this subtest, participants are presented with sets of seven pictures, and for each set, participants are instructed to circle the two drawings that are the most closely associated. They are given 3&#x02009;min to complete 40 printed sets. In accordance with WCJ-III standardized administration instructions, the Memory for Words and Auditory Working Memory subtests were not timed. The Decision Speed and Visual Matching subtests were timed tasks, but reaction time was not recorded. This screening battery provided baselines for the basic functions of processing speed and working memory using both visual and auditory tasks for each participant. Cognitive function inclusion criterion stipulated that participants needed to achieve standard scores greater than 85 on at least three of the four subtests to participate in the study. Participants completed a self-report, verifying normal or corrected vision (e.g., glasses or contacts). The standardized Edinburgh Handedness Inventory was used to establish participant handedness, wherein right-handedness is shown by a score &#x0003e;40.[<xref rid="ref24" ref-type="bibr">24</xref>] Twenty-six participants were right handed, two were ambidextrous, and one participant was left-handed. All participants signed consent forms prior to participating in the protocol. This project was approved by The University of Tennessee Health Science Center&#x02019;s Institutional Review Board.</p></sec></sec><sec id="sec1-3"><title>M<sc>aterials and</sc> P<sc>rocedure</sc></title><sec id="sec2-2"><title>General description of task conditions</title><p>All participants completed each of the experimental conditions. The three experimental conditions employed a DTP including both auditory and visual processing in noise. The simple dual-task condition examined visual processing speed and auditory speech perception. The moderate dual-task condition involved visual processing and auditory word recall (words in isolation). The complex dual-task condition examined visual processing and working memory for auditorily presented words in the context of a sentence. The conditions were created to denote a hierarchical progression in the complexity of each skill set, engaging working memory in the higher-level conditions.[<xref rid="ref1" ref-type="bibr">1</xref><xref rid="ref4" ref-type="bibr">4</xref><xref rid="ref5" ref-type="bibr">5</xref><xref rid="ref11" ref-type="bibr">11</xref><xref rid="ref25" ref-type="bibr">25</xref>] Task complexity was manipulated in the following two modalities: controlling the level of listening effort by increasing the amount of resources needed and the variation of the listening condition. The inclusion of the moderate task and 3-dB listening condition may provide insight into the more subtle changes to dual-task performance. The basic level in this progression was the simple dual-task condition, which involves completing a visual task and a word recognition task concurrently. Participants were asked to identify a word immediately after hearing it, which is considered a basic recognition task. In the moderate dual-task condition, participants were directed to maintain the words in memory prior to repeating them. This task taps into word recall and is more difficult than the simple dual-task condition. The complex dual-task condition was the most complex condition. During this task, participants were instructed to recall sets of words presented in sentences, in contrast to remembering words in isolation. All of these tasks were presented under varying signal-to-noise ratios (SNRs; 5, 3, and 0&#x02009;dB SNR). All SNR levels (5, 3, and 0&#x02009;dB SNR) were presented across each of the task conditions. Differences in accuracy and reaction time performance across the levels of SNR were calculated as the measures of listening effort. The experimental conditions are shown in <xref ref-type="table" rid="T1">Table 1</xref>.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Experimental conditions</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col span="1"/><col span="1"/><col span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Experimental conditions</th><th align="left" rowspan="1" colspan="1">Cognitive skills</th><th align="left" rowspan="1" colspan="1">Listening condition</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Simple dual task</td><td align="left" rowspan="1" colspan="1">Word identification + visual processing speed</td><td align="left" rowspan="1" colspan="1">0, 3, and 5&#x02009;dB SNR</td></tr><tr><td align="left" rowspan="1" colspan="1">Moderate dual task</td><td align="left" rowspan="1" colspan="1">Working memory (word list) + visual processing speed</td><td align="left" rowspan="1" colspan="1">0, 3, and 5&#x02009;dB SNR</td></tr><tr><td align="left" rowspan="1" colspan="1">Complex dual task</td><td align="left" rowspan="1" colspan="1">Working memory (sentences) + visual processing speed</td><td align="left" rowspan="1" colspan="1">0, 3, and 5&#x02009;dB SNR</td></tr></tbody></table></table-wrap><p>The least challenging SNR was 5&#x02009;dB, with the most difficult SNR at 0&#x02009;dB. During each dual task, complexity was modified by altering the auditory task, with the visual task remaining unaltered. All experimental conditions were randomized for each participant. As a result, some participants began with the complex dual-task condition, while others started with the simple or moderate dual-task conditions.</p></sec><sec id="sec2-3"><title>Auditory task: stimuli</title><p>Both the simple and complex dual-task conditions used the tape-recorded revised SPIN (SPIN-R) test materials[<xref rid="ref26" ref-type="bibr">26</xref><xref rid="ref27" ref-type="bibr">27</xref>] for the auditory stimuli. The SPIN-R materials include eight word lists (50 sentences each) equally balanced for contextual predictability. Both the simple and complex dual-task conditions required complete sentence lists. Three SPIN-R lists were assigned to each of the conditions, following the randomization of all eight lists. Therefore, each participant was assigned six randomized SPIN-R sentences.</p><p>The moderate dual-task condition utilized the audio-recorded CID W-22 materials.[<xref rid="ref28" ref-type="bibr">28</xref>] The CID W-22 materials consist of four recorded lists (50 words per list). While the CID W-22 lists originally included a carrier phrase before each word (e.g., &#x0201c;Say the word &#x02018;an&#x02019;&#x0201d;), the carrier phrase was eliminated. The target word was then the only word presented (e.g., &#x0201c;Day, Toe, Felt, Stove&#x0201d;). This modification was completed to account for any possible sentence context effects as a result from the carrier phrase.</p></sec><sec id="sec2-4"><title>Auditory task: procedure</title><p>The SNR conditions (5, 3, and 0&#x02009;dB SNR) were presented during each of the dual-task conditions. The SPIN-R multitalker bab;1;ble was utilized as the competing noise. Specifically, the noise was modified to effect the previously mentioned SNRs, with the presentation level for speech set at 65&#x02009;dB SPL. All testing was administered in a double-walled, sound-attenuating booth. The SPIN-R sentences, as well as the added CID W-22 word lists, were presented binaurally through insert earphones EAR-3A (Etymotic Research Inc., Elk Grove Village, IL, USA) routed through a Grason-Stadler 61 (Grason-Stadler Inc., Eden Prairie, MN, USA), two-channel audiometer. All of the following were randomized for each participant: experimental conditions (simple dual task, moderate dual task, and complex dual task), listening conditions (0, 3, and 5&#x02009;dB SNR), SPIN-R sentence list, and CID W-22 word list.</p></sec><sec id="sec2-5"><title>Visual task: stimuli</title><p>In addition to completing the auditory task, participants were asked to execute a visual task concurrently. Participants were directed to observe a visual field on a 15-inch computer monitor and indicate via button press when a target letter appeared in a randomized series of letters. All letters, created using the SuperLab 4.5 software,[<xref rid="ref29" ref-type="bibr">29</xref>] were presented in mid-field, with size 40 black font capital letters contrasted against a white background. The letters were presented on the screen for a maximum of 1500&#x02009;ms time limit or until the participant completed the button press, whichever occurred first. The letter was automatically erased after each letter stimulus. The visual task began with the auditory task, and continued until the auditory task was finished. Approximately 290 letters and the target letter appeared a minimum of 30 times and a maximum of 40 times during the simple dual-task condition. For the moderate and complex dual-task conditions, the presentation of the visual task stimuli differed in agreement with the length of the auditory task presentations. For example, in the moderate and complex dual-task condition, the participants were administered different numbers of stimuli depending upon their working memory capacity, whereas, for the simple dual-task condition, the entire list of 50 sentences was presented to all participants. Therefore, the visual task stimuli presentation length ranged in accordance with the number of stimuli administered during the working memory task.</p></sec><sec id="sec2-6"><title>Visual task: procedure</title><p>During each of the dual-task conditions, the three SNR conditions (5, 3, and 0&#x02009;dB SNR) were tested. A randomized target letter was selected for each of the SNR conditions. Participants were directed to monitor the computer screen and press the response button as soon as they identified the predetermined target letter. Each participant first received written instructions, which were verbally reinforced when the researcher read the instructions aloud. All participants indicated they understood the task prior to the start of the experiment. The instructions and target letter were presented on the computer screen prior to each listening condition. A small break was provided to reinstruct the participant with the new target letter for each listening condition (0, 3, and 5&#x02009;dB SNR). Visual processing speed (reaction time to letter presentation as measured by button press) and visual accuracy (correct identification of target letter) responses were recorded for each trial. Processing speed data, for accurate responses only, were examined during analyses.</p><p>This section has provided a description of the stimuli and procedures for the individual auditory and visual tasks. The details of each of the dual-task experimental conditions are in the following sections.</p></sec><sec id="sec2-7"><title>Simple dual-task condition</title><p>The simple dual-task condition consisted of performing the visual letter identification task while concurrently completing the auditory word recognition task in noise. Participants were instructed to repeat the final word heard in a sentence while simultaneously identifying a target letter presented in the visual display monitor. All participants received written and verbal instructions read aloud by the researcher. Participants indicated that they understood the task prior to the start of the experiment. All participants completed this dual task under all three SNR conditions. For each visual processing condition, we recorded visual processing speed and percent visual accuracy letter identification; for each listening condition, we recorded percent correct auditory word recognition.</p></sec><sec id="sec2-8"><title>Moderate dual-task condition</title><p>For the moderate dual-task condition, a working memory (isolated words) task and visual processing speed task were completed concurrently. Word list sequences presented words in list sets of 2, 4, 6, and 8 recall levels. For each level of recall (2, 4, 6, or 8), participants were given one set of word lists at a time. Two word list sets needed to be successfully completed prior to progressing to the next set level of recall. For example, a participant received one set of two words, and then was instructed to recall the set. Upon correct recall of the first set of two words, the participant would then receive the second set of two words. If the participant was able to correctly recall the second set of two words, then the set size level of recall would be complete. The set size of recall would then progress to four word sets. Thus, to progress to the next set size level of recall, the participant had to correctly recall both sets of words for each set level [i.e., 1 set of 2 (twice), 1 set of 4 (twice), 1 set of 6 (twice), and 1 set of 8 (twice)]. If one of the words in a set was incorrect, the set ended, and the next set condition was administered. Each set condition received a percent correct score. The set score was calculated by dividing the total number of words that were correctly recalled by the maximum number of words presented (32 words) times one hundred, for each participant. Potentially, participants could receive a maximum of 40 words, if they achieved the eight-item level. As a result, not all participants were presented with equal numbers of sentences. Visual processing measures included percent correct letter identification and processing speed for accurate visual responses.</p><p>In summary, participants were asked to listen to the presented word list set and then verbally recall each word heard in the set. Any order of recall was accepted, but participants were instructed to recall the word list set in order if possible. If the word list set was correctly recalled for both sets in the recall level, then the participant progressed to the next level in the word list set recall hierarchy. Additionally, the previously described visual processing task was simultaneously performed. All participants received written and verbal instructions read aloud by the researcher. Participants indicated that they understood the task prior to the start of the experiment.</p></sec><sec id="sec2-9"><title>Complex dual-task condition</title><p>The complex dual-task condition involved completing the Auditory Working Memory task (words in sentence context) and performing the visual task simultaneously. The procedures for this condition were adapted from Daneman and Carpenter[<xref rid="ref30" ref-type="bibr">30</xref>] procedures for assessing working memory and were used in the preliminary study. The SPIN-R sentences were utilized to examine working memory span for the sets of 2, 4, 6, and 8 sentences. The same procedures as described in the previous moderate dual-task condition section were used for the list sets of 2, 4, 6, and 8 recall levels. Participants were instructed to listen to a set of sentences and were asked to recall the final word of each of the sentences in the set. Participants needed to correctly recall both sets of final words in each sentence within each level. Again, similar to the moderate dual-task condition, if one of the words in a set was incorrectly recalled, the set ended, and the next set condition was administered. The scoring calculation was the same as described in the above section. As noted in earlier conditions, not all participants were presented with equal numbers of sentences. Visual processing measures included percent correct letter identification and processing speed (for accurate visual responses).</p><p>In summary, as in the moderate dual-task condition, participants were asked to listen to the presented sentence list set and then verbally recall each of the final words heard in the set. Participants were instructed to recall the word list set in order if possible, but any order was acceptable. If the sentence list set was correctly recalled for both sets in the recall level, then the participant progressed to the next level in list set recall hierarchy. Additionally, previously described visual processing task was simultaneously performed. All participants received written and verbal instructions read aloud by the researcher. Participants indicated that they understood the task prior to the start of the experiment.</p></sec></sec><sec sec-type="results" id="sec1-4"><title>R<sc>esults</sc></title><p>The current study investigated a relationship between cognitive function and performance during three levels of dual-task complexity (DT) (simple, moderate, and complex dual tasks) and by three levels of noise (5&#x02009;dB SNR, 3&#x02009;dB SNR, and 0 SNR). Three separate repeated measure ANOVAs were conducted for auditory accuracy, visual accuracy, and processing speed measures across DT, noise, and group conditions. Further, multivariate linear regression analysis was performed to predict dual-task performance from cognitive function. Arcsine transformation was conducted for all percent accuracy scores prior to analysis to stabilize the variance. The mean data (auditory and visual) for the simple, moderate, and complex DT conditions are shown in <xref ref-type="table" rid="T2">Table 2</xref>.</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Dual task auditory, visual accuracy, and speed average scores</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Listening condition</th><th align="center" colspan="5" rowspan="1">Auditory accuracy (%)</th><th align="center" colspan="2" rowspan="1">Visual accuracy (%)</th><th align="center" colspan="3" rowspan="1">Speed (ms)</th></tr><tr><th align="left" rowspan="1" colspan="1">
</th><th align="center" colspan="5" rowspan="1">
<hr/>
</th><th align="center" colspan="2" rowspan="1">
<hr/>
</th><th align="center" colspan="3" rowspan="1">
<hr/>
</th></tr><tr><th align="left" rowspan="1" colspan="1">
</th><th align="center" colspan="3" rowspan="1">Group 1</th><th align="center" colspan="2" rowspan="1">Group 2</th><th align="center" rowspan="1" colspan="1">Group 1</th><th align="center" rowspan="1" colspan="1">Group 2</th><th align="center" rowspan="1" colspan="1">Group 1</th><th align="center" colspan="2" rowspan="1">Group 2</th></tr></thead><tbody><tr><td align="left" colspan="11" rowspan="1">Simple</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02001;5 dB</td><td align="center" colspan="2" rowspan="1">96.80%</td><td align="center" colspan="3" rowspan="1">93.87%</td><td align="center" rowspan="1" colspan="1">93%</td><td align="center" rowspan="1" colspan="1">99%</td><td align="center" colspan="2" rowspan="1">610.09</td><td align="center" rowspan="1" colspan="1">593.92</td></tr><tr><td align="left" rowspan="1" colspan="1">
</td><td align="center" colspan="2" rowspan="1">(SD = 0.03)</td><td align="center" colspan="3" rowspan="1">(SD = 0.04)</td><td align="center" rowspan="1" colspan="1">(SD = 0.22)</td><td align="center" rowspan="1" colspan="1">(SD = 0.02)</td><td align="center" colspan="2" rowspan="1">(SD = 229.85)</td><td align="center" rowspan="1" colspan="1">(SD = 95.69)</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02001;3 dB</td><td align="center" colspan="2" rowspan="1">94.27%</td><td align="center" colspan="3" rowspan="1">91.20%</td><td align="center" rowspan="1" colspan="1">99%</td><td align="center" rowspan="1" colspan="1">97%</td><td align="center" colspan="2" rowspan="1">559.71</td><td align="center" rowspan="1" colspan="1">6176.18</td></tr><tr><td align="left" rowspan="1" colspan="1">
</td><td align="center" colspan="2" rowspan="1">(SD = 0.05)</td><td align="center" colspan="3" rowspan="1">(SD = 0.06)</td><td align="center" rowspan="1" colspan="1">(SD = 0.02)</td><td align="center" rowspan="1" colspan="1">(SD = 0.06)</td><td align="center" colspan="2" rowspan="1">(SD = 88.97)</td><td align="center" rowspan="1" colspan="1">(SD = 104.63)</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02001;0 dB</td><td align="center" colspan="2" rowspan="1">92.13%</td><td align="center" colspan="3" rowspan="1">88.67%</td><td align="center" rowspan="1" colspan="1">98%</td><td align="center" rowspan="1" colspan="1">98%</td><td align="center" colspan="2" rowspan="1">565.83</td><td align="center" rowspan="1" colspan="1">661.53</td></tr><tr><td align="left" rowspan="1" colspan="1">
</td><td align="center" colspan="2" rowspan="1">(SD = 0.05)</td><td align="center" colspan="3" rowspan="1">(SD = 0.06)</td><td align="center" rowspan="1" colspan="1">(SD = 0.03)</td><td align="center" rowspan="1" colspan="1">(SD = 0.04)</td><td align="center" colspan="2" rowspan="1">(SD = 95.14)</td><td align="center" rowspan="1" colspan="1">(SD = 216.69)</td></tr><tr><td align="left" colspan="11" rowspan="1">Moderate</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02001;5 dB</td><td align="center" colspan="4" rowspan="1">19.42%</td><td align="center" rowspan="1" colspan="1">16.46%</td><td align="center" rowspan="1" colspan="1">85%</td><td align="center" rowspan="1" colspan="1">94%</td><td align="center" colspan="2" rowspan="1">787.39</td><td align="center" rowspan="1" colspan="1">721.80</td></tr><tr><td align="left" rowspan="1" colspan="1">
</td><td align="center" colspan="4" rowspan="1">(SD = 24.58)</td><td align="center" rowspan="1" colspan="1">(SD = 20.07)</td><td align="center" rowspan="1" colspan="1">(SD = 0.21)</td><td align="center" rowspan="1" colspan="1">(SD = 0.10)</td><td align="center" colspan="2" rowspan="1">(SD = 223.38)</td><td align="center" rowspan="1" colspan="1">(SD = 130.78)</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02001;3 dB</td><td align="center" colspan="4" rowspan="1">8.26%</td><td align="center" rowspan="1" colspan="1">16.30%</td><td align="center" rowspan="1" colspan="1">90%</td><td align="center" rowspan="1" colspan="1">91%</td><td align="center" colspan="2" rowspan="1">726.39</td><td align="center" rowspan="1" colspan="1">788.26</td></tr><tr><td align="left" rowspan="1" colspan="1">
</td><td align="center" colspan="4" rowspan="1">(SD = 5.96)</td><td align="center" rowspan="1" colspan="1">(SD = 12.51)</td><td align="center" rowspan="1" colspan="1">(SD = 0.18)</td><td align="center" rowspan="1" colspan="1">(SD = 0.18)</td><td align="center" colspan="2" rowspan="1">(SD = 173.52)</td><td align="center" rowspan="1" colspan="1">(SD = 270.48)</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02001;0 dB</td><td align="center" colspan="4" rowspan="1">18.97%</td><td align="center" rowspan="1" colspan="1">10.21%</td><td align="center" rowspan="1" colspan="1">88%</td><td align="center" rowspan="1" colspan="1">75%</td><td align="center" colspan="2" rowspan="1">725.64</td><td align="center" rowspan="1" colspan="1">839.98</td></tr><tr><td align="left" rowspan="1" colspan="1">
</td><td align="center" colspan="4" rowspan="1">(SD = 0.15)</td><td align="center" rowspan="1" colspan="1">(SD = 9.02)</td><td align="center" rowspan="1" colspan="1">(SD = 0.22)</td><td align="center" rowspan="1" colspan="1">(SD = 0.38)</td><td align="center" colspan="2" rowspan="1">(SD = 281.106)</td><td align="center" rowspan="1" colspan="1">(SD = 348.45)</td></tr><tr><td align="left" colspan="11" rowspan="1">Complex</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02001;5 dB</td><td align="center" rowspan="1" colspan="1">29.46%</td><td align="center" colspan="4" rowspan="1">21.04%</td><td align="center" rowspan="1" colspan="1">79.89%</td><td align="center" rowspan="1" colspan="1">77.69%</td><td align="center" colspan="2" rowspan="1">829.85</td><td align="center" rowspan="1" colspan="1">786.37</td></tr><tr><td align="left" rowspan="1" colspan="1">
</td><td align="center" rowspan="1" colspan="1">(SD = 20.61)</td><td align="center" colspan="4" rowspan="1">(SD = 7.86)</td><td align="center" rowspan="1" colspan="1">(SD = 0.35)</td><td align="center" rowspan="1" colspan="1">(SD = 0.32)</td><td align="center" colspan="2" rowspan="1">(SD = 322.92)</td><td align="center" rowspan="1" colspan="1">(SD = 249.73)</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02001;3 dB</td><td align="center" colspan="4" rowspan="1">24.33%</td><td align="center" rowspan="1" colspan="1">21.88%</td><td align="center" rowspan="1" colspan="1">94.52%</td><td align="center" rowspan="1" colspan="1">80.03%</td><td align="center" colspan="2" rowspan="1">666.85</td><td align="center" rowspan="1" colspan="1">799.83</td></tr><tr><td align="left" rowspan="1" colspan="1">
</td><td align="center" colspan="4" rowspan="1">(SD = 11.70)</td><td align="center" rowspan="1" colspan="1">(SD = 11.88)</td><td align="center" rowspan="1" colspan="1">(SD = 0.08)</td><td align="center" rowspan="1" colspan="1">(SD = 0.27)</td><td align="center" colspan="2" rowspan="1">(SD = 132.65)</td><td align="center" rowspan="1" colspan="1">(SD = 232.93)</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02001;0 dB</td><td align="center" colspan="4" rowspan="1">29.02%</td><td align="center" rowspan="1" colspan="1">20.83%</td><td align="center" rowspan="1" colspan="1">88.38%</td><td align="center" rowspan="1" colspan="1">80.30%</td><td align="center" colspan="2" rowspan="1">698.88</td><td align="center" rowspan="1" colspan="1">763.91</td></tr><tr><td align="left" rowspan="1" colspan="1">
</td><td align="center" colspan="4" rowspan="1">(SD = 23.75)</td><td align="center" rowspan="1" colspan="1">(SD = 12.83)</td><td align="center" rowspan="1" colspan="1">(SD = 0.096)</td><td align="center" rowspan="1" colspan="1">(SD = 0.25)</td><td align="center" colspan="2" rowspan="1">(SD = 147.828)</td><td align="center" rowspan="1" colspan="1">(SD = 250.97)</td></tr></tbody></table></table-wrap><sec id="sec2-10"><title>Repeated measure analysis of variance</title><p>The repeated measures analysis of variance was performed with DT (simple, moderate, and complex dual tasks) and noise (5&#x02009;dB, 3&#x02009;dB, and 0&#x02009;dB SNR) as within-subject effects and group (Group 1: younger; Group 2: older) as a between-subject effect. There is a significant omnibus effect of DT [Wilks&#x02019; Lambda <italic>F</italic>(6,23)&#x02009;=&#x02009;367.14, <italic>P</italic>&#x02009;&#x02264;&#x02009;0.001, <italic>&#x003b7;</italic>
<sup>2</sup>&#x02009;=&#x02009;0.990]. There was no significant omnibus effect of noise or group. As a result of the small sample size, interaction effects were not explored. The effect of task on auditory accuracy, visual accuracy, and visual reaction time will be explored next.</p><sec id="sec3-1"><title>Auditory accuracy</title><p>A significant main effect of DT was found on auditory accuracy measures [<italic>F</italic>(1.71,47.81)&#x02009;=&#x02009;963.15, <italic>P</italic>&#x02009;&#x02264;&#x02009;0.001 <italic>&#x003b7;</italic>
<sup>2</sup>&#x02009;=&#x02009;0.972 (Greenhouse&#x02013;Geisser Adjustment)]. To further examine this effect, a test of within-subject contrasts was conducted. Task was significant for auditory accuracy, with differences between simple DT and complex DT [<italic>F</italic>(1,28)&#x02009;=&#x02009;1628, <italic>P</italic>&#x02009;&#x02264;&#x02009;0.001, <italic>&#x003b7;</italic>
<sup>2</sup>&#x02009;=&#x02009;0.983], as well as moderate DT and complex DT [<italic>F</italic>(1,28)&#x02009;=&#x02009;14.906, <italic>P</italic>&#x02009;=&#x02009;0.001, <italic>&#x003b7;</italic>
<sup>2</sup>&#x02009;=&#x02009;0.347]. Pairwise comparisons with Bonferroni corrections confirm that participants were more accurate during the simple DT (<italic>M</italic>&#x02009;=&#x02009;1.23) and moderate DT (<italic>M</italic>&#x02009;=&#x02009;0.153) tasks compared to the complex DT (<italic>M</italic>&#x02009;=&#x02009;0.246) condition. Refer <xref ref-type="fig" rid="F1">Figure 1</xref> for details. <xref ref-type="fig" rid="F1">Figure 1</xref> includes transformed data, not percentage data.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p>Pairwise means in auditory accuracy task performance. Error bars represent the standard error of the mean</p></caption><graphic xlink:href="NH-19-254-g001"/></fig></sec><sec id="sec3-2"><title>Visual accuracy</title><p>A significant main effect of DT was found on visual accuracy [<italic>F</italic>(1.91,53.46)&#x02009;=&#x02009;15.23, <italic>P</italic>&#x02009;&#x02264;&#x02009;0.001, <italic>&#x003b7;</italic>
<sup>2</sup>&#x02009;=&#x02009;0.352 (Greenhouse&#x02013;Geisser Adjustment)], with the within-subject contrast revealing differences between simple DT and complex DT [<italic>F</italic>(1,28)&#x02009;=&#x02009;28.94, <italic>P</italic>&#x02009;&#x02264;&#x02009;0.001, <italic>&#x003b7;</italic>
<sup>2</sup>&#x02009;=&#x02009;0.508], as well as moderate DT and complex DT [<italic>F</italic>(1,28)&#x02009;=&#x02009;5.01, <italic>P</italic>&#x02009;&#x02264;&#x02009;0.033, <italic>&#x003b7;</italic>
<sup>2</sup>&#x02009;=&#x02009;0.152]. Using pairwise comparisons with Bonferroni corrections, it was found that the pairwise comparisons for visual accuracy were statistically significant (<italic>P</italic> &#x0003c; 0.05). Results suggest that in the simple DT (<italic>M</italic>&#x02009;=&#x02009;1.45) and moderate DT (<italic>M</italic>&#x02009;=&#x02009;1.26) levels, participants were more accurate than in the complex DT (<italic>M</italic>&#x02009;=&#x02009;1.16) levels. Refer <xref ref-type="fig" rid="F2">Figure 2</xref> for details. <xref ref-type="fig" rid="F2">Figure 2</xref> includes transformed data, not percentage data. These findings indicate that simple DT was significantly different from both moderate DT and complex DT.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p>Pairwise means in visual accuracy for task performance. Error bars represent the standard error of the mean</p></caption><graphic xlink:href="NH-19-254-g002"/></fig></sec><sec id="sec3-3"><title>Visual processing speed</title><p>Finally, a significant main effect of task was found on visual processing speed [<italic>F</italic>(1.83,50.14)&#x02009;=&#x02009;348.48, <italic>P</italic>&#x02009;&#x02264;&#x02009;0.001, <italic>&#x003b7;</italic>
<sup>2</sup>&#x02009;=&#x02009;0.926 (Greenhouse&#x02013;Geisser Adjustment)]. The within-subject contrast revealed that differences between simple DT and complex DT [<italic>F</italic>(1,28)&#x02009;=&#x02009;515.32, <italic>P</italic>&#x02009;&#x02264;&#x02009;0.001, <italic>&#x003b7;</italic>
<sup>2</sup>&#x02009;=&#x02009;0.948] were statistically significant. Pairwise comparisons with Bonferroni corrections indicated that the simple task was significantly different from the complex tasks. However, the visual processing speed for the moderate task was not significantly different from the complex task. These differences suggest that for the simple DT participants were faster (<italic>M</italic>&#x02009;=&#x02009;600) than at the moderate DT (<italic>M</italic>&#x02009;=&#x02009;765.94) and the complex DT (<italic>M</italic>&#x02009;=&#x02009;758.50) levels. Refer <xref ref-type="fig" rid="F3">Figure 3</xref> for details.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p>Pairwise means in visual processing speed for task performance. Error bars represent the standard error of the mean</p></caption><graphic xlink:href="NH-19-254-g003"/></fig></sec></sec><sec id="sec2-11"><title>Multivariate linear regression analysis</title><p>A multivariate linear regression analysis was performed to explore whether performance during participation in complex tasks (auditory accuracy, visual accuracy, and visual reaction time) can be predicted from cognitive function as measured by the subtests of the WCJ-III test (Memory for Words, Auditory Working Memory, Visual Matching, and Decision Speed). The Memory for Words subtest predicted moderate DT auditory accuracy performance, with <italic>F</italic>(1,24)&#x02009;=&#x02009;5.218, <italic>P</italic>&#x02009;=&#x02009;0.031, <italic>R</italic>
<sup>2</sup>&#x02009;=&#x02009;0.216, only, suggesting that the more accurate the performance in Memory for Words, the greater auditory accuracy during the moderate dual-task condition. The Auditory Working Memory subtest predicted complex DT auditory accuracy performance [<italic>F</italic>(1,24)&#x02009;=&#x02009;5.051, <italic>P</italic>&#x02009;=&#x02009;0.034, <italic>R</italic>
<sup>2</sup>&#x02009;=&#x02009;0.549], indicating that increased Auditory Working Memory performance results in more accurate auditory accuracy in the complex dual-task condition. The Decision Speed subtest predicted simple DT visual accuracy performance [<italic>F</italic>(1,24)&#x02009;=&#x02009;7.485, <italic>P</italic>&#x02009;=&#x02009;0.012, <italic>R</italic>
<sup>2</sup>&#x02009;=&#x02009;0.420] and complex DT visual accuracy performance [<italic>F</italic>(1,24)&#x02009;=&#x02009;4.272, <italic>P</italic>&#x02009;=&#x02009;0.050, <italic>R</italic>
<sup>2</sup>&#x02009;=&#x02009;0.328], suggesting that as Decision Speed gets faster, visual accuracy increases in the simple and complex visual tasks. Decision Speed also negatively predicted complex DT visual reaction time performance [<italic>F</italic>(1,24)&#x02009;=&#x02009;6.156, <italic>P</italic>&#x02009;=&#x02009;0.020, <italic>R</italic>
<sup>2</sup>&#x02009;=&#x02009;0.376], indicating that as Decision Speed score increases, visual reaction time decreases. Thus, participants who completed more items tended to also score higher on complex visual reaction time tasks. Finally, the Visual Matching subtest predicted simple DT auditory accuracy performance [<italic>F</italic>(1,24)&#x02009;=&#x02009;6.511, <italic>P</italic>&#x02009;=&#x02009;0.018, <italic>R</italic>
<sup>2</sup>&#x02009;=&#x02009;0.236]. In summary, the following subtests of the WCJ-III test, Memory for Words, Auditory Working Memory, and Visual Matching all predicted auditory accuracy performance; however, the Auditory Working Memory subtests was the strongest predictor of auditory accuracy, yielding the larger <italic>R</italic>
<sup>2</sup>&#x02009;=&#x02009;0.549 at the complex dual task. Further, Decision Speed predicted performance for both visual accuracy and visual reaction time, but inconsistently across task complexity.</p></sec></sec><sec sec-type="discussion" id="sec1-5"><title>D<sc>iscussion</sc></title><p>This study was designed to investigate whether task complexity affects cognitive load and if cognitive function subtests may predict performance during complex tasks. We first discuss the results of the group data followed by the regression findings. The results of the group analysis suggest that task complexity has a significant effect on auditory accuracy, visual accuracy, and visual processing speed, indicating that listening effort or cognitive load was successfully manipulated by the different levels of task. In general, auditory and visual accuracy decreased and reaction time increased with increased task complexity, suggesting increased cognitive load across tasks. During the simple dual task, auditory accuracy was not negatively affected. However, as task complexity increased, auditory accuracy decreased significantly.</p><p>This suggests that a more challenging primary task is necessary to elucidate which cognitive factors are important in explaining individual differences in auditory accuracy.</p><p>Task complexity had a more predictable effect on visual accuracy, because visual accuracy decreased with each increase in complexity. Additionally, task complexity also significantly affected visual processing speed, showing that as task complexity increased, processing speed decreased. The investigation of dual-task performance is important for the study of listening effort across age groups and cognitive skill.[<xref rid="ref11" ref-type="bibr">11</xref><xref rid="ref14" ref-type="bibr">14</xref>] The results of this study highlight the fact that task complexity or difficulty significantly affects the results of dual-task methodologies in listeners with normal cognitive screening results and near-normal hearing. Additionally, a simple dual task may not show cognitive loading if it does not challenge the listener significantly, and, subsequently, performance may be overestimated. By including more challenging dual tasks, investigators may begin to see the level of complexity at which performance may start to break down, thus providing a deeper understanding of function during more challenging tasks. These findings support previous literature that show that listening effort is increased in DTPs and contributes to the understanding of cognitive function across task complexity. On the basis of these findings, cognitive processing breakdown occurs between simple dual-task condition and the moderate and/or complex dual-task conditions.</p><p>Unexpectedly, the analysis of the group data did not reveal an effect of age on cognitive load. This is surprising because previous research[<xref rid="ref1" ref-type="bibr">1</xref><xref rid="ref5" ref-type="bibr">5</xref><xref rid="ref20" ref-type="bibr">20</xref><xref rid="ref31" ref-type="bibr">31</xref>] has found that older listeners show increased listening effort compared to younger listeners. However, in many of these studies, the younger listeners are often college-aged students (18&#x02013;25 years old), increasing the likelihood of finding an age effect.[<xref rid="ref12" ref-type="bibr">12</xref>] In the current study, listeners in the young group (Group 1) ranged in age from 41 to 61 years, whereas the older listeners were from 63 to 81. This covers a smaller range of age and leaves a smaller age gap between groups compared to other studies.[<xref rid="ref1" ref-type="bibr">1</xref><xref rid="ref5" ref-type="bibr">5</xref><xref rid="ref31" ref-type="bibr">31</xref>] The reason for the different age selection here was to maximize variation in cognitive function within the normal range for older adults. However, Degeest <italic>et al</italic>.[<xref rid="ref20" ref-type="bibr">20</xref>] did find an effect of age with a similar separation between age groups. Zekveld <italic>et al</italic>.[<xref rid="ref17" ref-type="bibr">17</xref>] have discussed this limitation for a similar subject design for a speech-perception-in-noise pupil-dilation study in older and middle-aged adults. As a result of the continued change of cognition and neurocognitive processes with age, appropriate age selection criteria are unclear. Choosing a larger age separation and including a young adult age group may have revealed a significant age effect. Regardless, age effects likely include a strong cognitive component, as discussed in the following section. Second, all of the participants included in this study demonstrated normal cognition within the cognitive skills contributing to listening effort. Of the studies that have shown an effect of age on cognitive load, some have used cognitive screens only for the older participants,[<xref rid="ref20" ref-type="bibr">20</xref>] and others have used abbreviated cognitive screens that may not have been sensitive to listening effort.[<xref rid="ref1" ref-type="bibr">1</xref><xref rid="ref20" ref-type="bibr">20</xref>] Thus, it is unclear whether age differences found in some of the studies may be due to a lack of sensitivity of cognitive screening protocol, thereby potentially including some participants with possible cognitive decline in the older group. Furthermore, including much younger participants as the control group may conflate age with changes in hearing.A second goal of the current study was to explore whether performance on the dual tasks could be predicted from the WCJ-III battery. The current results suggest that the WCJ-III test of cognitive abilities subtests Memory for Words, Auditory Working Memory, Visual Matching, and Decision Speed each predict performance on dual tasks, regardless of age. Specifically, each of the subtests predicts different levels of dual-task performance across task complexity. Auditory accuracy, across all levels of DT, was predicted by Memory for Words, Auditory Working Memory, and Visual Matching. Further, Decision Speed predicted visual accuracy performance at the simple DT level and visual reaction time in both the simple and complex DT levels. These results suggest that examining participant cognitive function across working memory and processing speed may predict performance across dual tasks of varying complexity. Because we only included participants who had normal cognitive function, the relationship between these variables may become stronger as participants with declining cognitive function are included.</p><p>The dual-task hierarchy we designed for this study warrants review. While the purpose of the current study was to investigate a possible predictive relationship cognitive function and dual-task performance, the use of dual-task methodology remains highly variable across fields and merits further exploration. The current study included recall of words in isolation part of the moderate DT, with recall of words in a sentence part of the complex DT. We reasoned that recall of words in a sentence would be more difficult than recall of words in isolation, because cognitive processing of sentences may require additional resources. However, our results indicate that the recall of words in isolation may be more challenging in sentence context, because no sentence context was used to guide perception. Further investigation of more challenging levels of dual-task hierarchy may inform researchers and clinicians regarding multitasking performance during tasks of everyday life across varying populations and age groups. In additional to dual-task hierarchy, further investigation with a larger sample size is warranted. Despite the small sample size, which limits statistical power, significant prediction of dual-task performance was found, which addresses the aim of the study.</p><sec id="sec2-12"><title>Implications of results</title><p>As our understanding of the relationship between cognition and listening effort improves, it becomes increasingly apparent that an audiological test battery may need to include a cognitive screening to better understand daily function. This study identifies four cognitive subtests which, in combination, significantly predict auditory accuracy, visual accuracy, and visual reaction time during dual-task performance of varying complexity. These findings may inform both researchers and clinicians regarding the relationship between cognitive screens and listening effort when working with individuals during complex speech recognition in noise tasks. Further, this finding highlights that collaborative relationships between speech-language pathologist and audiologist are likely needed to better understand the communicative capacity of participants.</p></sec><sec id="sec2-13"><title>Financial support and sponsorship</title><p>Nil.</p></sec><sec id="sec2-14" sec-type="COI-statement"><title>Conflicts of interest</title><p>There are no conflicts of interest.</p></sec></sec></body><back><ref-list><title>R<sc>EFERENCES</sc></title><ref id="ref1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosselin</surname><given-names>PA</given-names></name><name><surname>Gagn&#x000e9;</surname><given-names>JP</given-names></name></person-group><article-title>Older adults expend more listening effort than younger adults recognizing speech in noise</article-title><source>J Speech Lang Hear Res</source><year>2011</year><volume>54</volume><fpage>944</fpage><lpage>58</lpage><pub-id pub-id-type="pmid">21060138</pub-id></element-citation></ref><ref id="ref2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fraser</surname><given-names>S</given-names></name><name><surname>Gagn&#x000e9;</surname><given-names>JP</given-names></name></person-group><article-title>Evaluating the effort expended to understand speech in noise using a dual task paradigm: The effects of providing visual speech cues</article-title><source>J Speech Lang Hear Res</source><year>2010</year><volume>53</volume><fpage>18</fpage><lpage>33</lpage><pub-id pub-id-type="pmid">19635945</pub-id></element-citation></ref><ref id="ref3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>MK</given-names></name></person-group><article-title>How cognition might influence hearing aid-design, fitting, and outcomes</article-title><source>Hear J</source><year>2009</year><volume>62</volume><fpage>32, 34, 36, 38</fpage></element-citation></ref><ref id="ref4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarampalis</surname><given-names>A</given-names></name><name><surname>Kalluri</surname><given-names>S</given-names></name><name><surname>Edwards</surname><given-names>B</given-names></name><name><surname>Hafter</surname><given-names>E</given-names></name></person-group><article-title>Objective measures of listening effort: Effects of background noise and noise reduction</article-title><source>J Speech Lang Hear Res</source><year>2009</year><volume>52</volume><fpage>1230</fpage><lpage>40</lpage><pub-id pub-id-type="pmid">19380604</pub-id></element-citation></ref><ref id="ref5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tun</surname><given-names>PA</given-names></name><name><surname>McCoy</surname><given-names>S</given-names></name><name><surname>Wingfield</surname><given-names>A</given-names></name></person-group><article-title>Aging, hearing acuity, and the attentional costs of effortful listening</article-title><source>Psychol Aging</source><year>2009</year><volume>24</volume><fpage>761</fpage><lpage>6</lpage><pub-id pub-id-type="pmid">19739934</pub-id></element-citation></ref><ref id="ref6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akeryod</surname><given-names>MA</given-names></name></person-group><article-title>Are individual differences in speech reception related to individual differences in cognitive ability? A survey of twenty experimental studies with normal and hearing-impaired adults</article-title><source>Int J Audiol</source><year>2008</year><issue>Suppl 2</issue><fpage>S53</fpage><lpage>71</lpage><pub-id pub-id-type="pmid">19012113</pub-id></element-citation></ref><ref id="ref7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCoy</surname><given-names>SL</given-names></name><name><surname>Tun</surname><given-names>PA</given-names></name><name><surname>Cox</surname><given-names>LC</given-names></name><name><surname>Colangelo</surname><given-names>M</given-names></name><name><surname>Stewart</surname><given-names>RA</given-names></name><name><surname>Wingfield</surname><given-names>A</given-names></name></person-group><article-title>Hearing loss and perceptual effort: Downstream effects on older adults&#x02019; memory for speech</article-title><source>Q J Exp Psychol</source><year>2005</year><volume>58</volume><fpage>22</fpage><lpage>33</lpage></element-citation></ref><ref id="ref8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>MK</given-names></name><name><surname>Schneider</surname><given-names>BA</given-names></name><name><surname>Daneman</surname><given-names>M</given-names></name></person-group><article-title>How young and old adults listen to and remember speech in noise</article-title><source>J Acoust Soc Am</source><year>1995</year><volume>97</volume><fpage>593</fpage><lpage>608</lpage><pub-id pub-id-type="pmid">7860836</pub-id></element-citation></ref><ref id="ref9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salthouse</surname><given-names>T</given-names></name></person-group><article-title>When does age-related cognitive decline begin</article-title><source>Neurobiol Aging</source><year>2009</year><volume>30</volume><fpage>507</fpage><lpage>14</lpage><pub-id pub-id-type="pmid">19231028</pub-id></element-citation></ref><ref id="ref10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downs</surname><given-names>DW</given-names></name></person-group><article-title>Effects of hearing aid use on speech discrimination and listening effort</article-title><source>J Speech Hear Disord</source><year>1982</year><volume>47</volume><fpage>189</fpage><lpage>93</lpage><pub-id pub-id-type="pmid">7176597</pub-id></element-citation></ref><ref id="ref11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosselin</surname><given-names>P</given-names></name><name><surname>Gagn&#x000e9;</surname><given-names>JP</given-names></name></person-group><article-title>Use of dual task paradigm to measure listening effort</article-title><source>Can J Speech Lang Pathol Audiol</source><year>2010</year><volume>34</volume><fpage>43</fpage><lpage>51</lpage></element-citation></ref><ref id="ref12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desjardins</surname><given-names>JL</given-names></name><name><surname>Doherty</surname><given-names>KA</given-names></name></person-group><article-title>The effect of hearing aid noise reduction on listening effort in hearing-impaired adults</article-title><source>Ear Hear</source><year>2014</year><volume>35</volume><fpage>600</fpage><lpage>10</lpage><pub-id pub-id-type="pmid">24622352</pub-id></element-citation></ref><ref id="ref13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holtzer</surname><given-names>R</given-names></name><name><surname>Stern</surname><given-names>Y</given-names></name><name><surname>Rakit</surname><given-names>B</given-names></name></person-group><article-title>Predicting age-related dual-task effects with individual differences on neuropsychological tests</article-title><source>Neuropsychology</source><year>2005</year><volume>19</volume><fpage>18</fpage><lpage>27</lpage><pub-id pub-id-type="pmid">15656759</pub-id></element-citation></ref><ref id="ref14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picou</surname><given-names>EM</given-names></name><name><surname>Ricketts</surname><given-names>TA</given-names></name></person-group><article-title>The effect of changing the secondary task in dual-task paradigms for measuring listening effort</article-title><source>Ear Hear</source><year>2014</year><volume>35</volume><fpage>611</fpage><lpage>22</lpage><pub-id pub-id-type="pmid">24992491</pub-id></element-citation></ref><ref id="ref15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picou</surname><given-names>EM</given-names></name><name><surname>Ricketts</surname><given-names>TA</given-names></name><name><surname>Hornsby</surname><given-names>BW</given-names></name></person-group><article-title>Visual cues and listening effort: Individual variability</article-title><source>J Speech Lang Hear Res</source><year>2011</year><volume>54</volume><fpage>1416</fpage><lpage>30</lpage><pub-id pub-id-type="pmid">21498576</pub-id></element-citation></ref><ref id="ref16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudner</surname><given-names>M</given-names></name><name><surname>Lunner</surname><given-names>T</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Sundewall-Thoren</surname><given-names>E</given-names></name><name><surname>Ronnberg</surname><given-names>J</given-names></name></person-group><article-title>Working memory capacity may influence perceived effort during aided speech recognition in noise</article-title><source>J Am Acad Audiol</source><year>2012</year><volume>23</volume><fpage>577</fpage><lpage>89</lpage><pub-id pub-id-type="pmid">22967733</pub-id></element-citation></ref><ref id="ref17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname><given-names>AA</given-names></name><name><surname>Kramer</surname><given-names>SE</given-names></name><name><surname>Festen</surname><given-names>JM</given-names></name></person-group><article-title>Cognitive load during speech perception in noise: The influence of age, hearing loss, and cognition on the pupil response</article-title><source>Ear Hear</source><year>2011</year><volume>32</volume><fpage>498</fpage><lpage>510</lpage><pub-id pub-id-type="pmid">21233711</pub-id></element-citation></ref><ref id="ref18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname><given-names>AA</given-names></name><name><surname>Festen</surname><given-names>JM</given-names></name><name><surname>Kramer</surname><given-names>SE</given-names></name></person-group><article-title>Task difficulty differentially affects two measures of processing load: The pupil response during sentence processing and delayed cued recall of the sentences</article-title><source>J Speech Lang Hear Res</source><year>2013</year><volume>56</volume><fpage>1156</fpage><lpage>65</lpage><pub-id pub-id-type="pmid">23785182</pub-id></element-citation></ref><ref id="ref19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasreddine</surname><given-names>ZS</given-names></name><name><surname>Phillips</surname><given-names>NA</given-names></name><name><surname>Bedirian</surname><given-names>V</given-names></name><name><surname>Charbonneau</surname><given-names>S</given-names></name><name><surname>Whitehead</surname><given-names>V</given-names></name><name><surname>Collin</surname><given-names>I</given-names></name><etal/></person-group><article-title>The Montr&#x000e9;al Cognitive Assessment, MoCA: A brief screening tool for mild cognitive impairment</article-title><source>J Am Geriatr Soc</source><year>2005</year><volume>53</volume><fpage>695</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">15817019</pub-id></element-citation></ref><ref id="ref20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degeest</surname><given-names>S</given-names></name><name><surname>Keppler</surname><given-names>H</given-names></name><name><surname>Corthals</surname><given-names>P</given-names></name></person-group><article-title>The effect of age on listening effort</article-title><source>J Speech Lang Hear Res</source><year>2015</year><volume>58</volume><fpage>1592</fpage><lpage>600</lpage><pub-id pub-id-type="pmid">26161899</pub-id></element-citation></ref><ref id="ref21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh-Manoux</surname><given-names>A</given-names></name><name><surname>Kivimaki</surname><given-names>M</given-names></name><name><surname>Glymour</surname><given-names>MM</given-names></name><name><surname>Elbaz</surname><given-names>A</given-names></name><name><surname>Berr</surname><given-names>C</given-names></name><name><surname>Ebmeier</surname><given-names>KP</given-names></name><etal/></person-group><article-title>Timing of onset of cognitive decline: Results from Whitehall II prospective cohort study</article-title><source>BMJ</source><year>2012</year><volume>344</volume><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="ref22"><label>22</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Woodcock</surname><given-names>RW</given-names></name><name><surname>Mather</surname><given-names>N</given-names></name><name><surname>McGrew</surname><given-names>KS</given-names></name></person-group><source>Woodcock-Johnson III Tests of Cognitive Abilities &#x02013; Examiner&#x02019;s Manual</source><year>2001</year><publisher-loc>Itasca</publisher-loc><publisher-name>Riverside</publisher-name></element-citation></ref><ref id="ref23"><label>23</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schrank</surname><given-names>FA</given-names></name><name><surname>Wendling</surname><given-names>BJ</given-names></name></person-group><source>Educational interventions and accommodations related to the Woodcock-Johnson<sup>&#x000ae;</sup> III Tests of Cognitive Abilities and the Woodcock-Johnson III Diagnostic Supplement to the Tests of Cognitive Abilities (Woodcock-Johnson III Assessment Service Bulletin No. 10)</source><year>2009</year><publisher-loc>Rolling Meadows, IL</publisher-loc><publisher-name>Riverside Publishing</publisher-name></element-citation></ref><ref id="ref24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname><given-names>RC</given-names></name></person-group><article-title>The assessment and analysis of handedness: The Edinburgh Inventory</article-title><source>Neuropsychologica</source><year>1971</year><volume>9</volume><fpage>97</fpage><lpage>113</lpage></element-citation></ref><ref id="ref25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hornsby</surname><given-names>BW</given-names></name></person-group><article-title>The effects of hearing aid use on listening effort and mental fatigue associated with sustained processing demands</article-title><source>Ear Hear</source><year>2013</year><volume>34</volume><fpage>523</fpage><lpage>34</lpage><pub-id pub-id-type="pmid">23426091</pub-id></element-citation></ref><ref id="ref26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bilger</surname><given-names>RC</given-names></name><name><surname>Neutzel</surname><given-names>JM</given-names></name><name><surname>Rabinowitz</surname><given-names>WM</given-names></name><name><surname>Rzeczkowski</surname><given-names>C</given-names></name></person-group><article-title>Standardization of a test of speech perception in noise</article-title><source>J Speech Lang Hear Res</source><year>1984</year><volume>27</volume><fpage>32</fpage><lpage>48</lpage></element-citation></ref><ref id="ref27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalikow</surname><given-names>DN</given-names></name><name><surname>Stevens</surname><given-names>KN</given-names></name><name><surname>Elliott</surname><given-names>LL</given-names></name></person-group><article-title>Development of a test of speech intelligibility in noise using sentence materials with controlled word predictability</article-title><source>J Acoust Soc Am</source><year>1977</year><volume>61</volume><fpage>1337</fpage><lpage>51</lpage><pub-id pub-id-type="pmid">881487</pub-id></element-citation></ref><ref id="ref28"><label>28</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hirsh</surname><given-names>IJ</given-names></name></person-group><source>The Measurement of Hearing</source><year>1952</year><publisher-loc>New York, NY</publisher-loc><publisher-name>McGraw Hill</publisher-name></element-citation></ref><ref id="ref29"><label>29</label><element-citation publication-type="book"><collab>Cedrus Corporation</collab><source>SuperLab (Version 4.5) [Computer Software]</source><year>2008</year><publisher-loc>United States</publisher-loc><publisher-name>Cedrus Corporation</publisher-name></element-citation></ref><ref id="ref30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daneman</surname><given-names>M</given-names></name><name><surname>Carpenter</surname><given-names>PA</given-names></name></person-group><article-title>Individual differences in working memory and reading</article-title><source>J Verbal Learn Verbal Behav</source><year>1980</year><volume>19</volume><fpage>450</fpage><lpage>66</lpage></element-citation></ref><ref id="ref31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desjardins</surname><given-names>JL</given-names></name><name><surname>Doherty</surname><given-names>KA</given-names></name></person-group><article-title>Age-related changes in listening effort for various types of masker noises</article-title><source>Ear Hear</source><year>2013</year><volume>34</volume><fpage>261</fpage><lpage>72</lpage><pub-id pub-id-type="pmid">23095723</pub-id></element-citation></ref></ref-list></back></article>